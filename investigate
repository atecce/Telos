#!/usr/bin/env python
#
# I should not like my writing to spare other people the trouble of thinking. 
# But, if possible, to stimulate someone to thoughts of their own.
#

# need these to get and parse html
from bs4 import BeautifulSoup
import requests

# set up main page
root_url  = 'http://www.lyrics.net'
main_page = requests.get(root_url)
main_soup = BeautifulSoup(main_page.content, 'lxml')

# keep this to skip collected urls
subalphabet_urls = list()

# crawl the main page
for link in main_soup.find_all('a'):

	# filter out unwanted links
	if '.php' in link.get('href'): continue 
	if 'http' in link.get('href'): continue

	# the rest are alphabet suburls
	alphabet_suburl = link.get('href')

	# only get links relevant to artists
	if 'artists' in alphabet_suburl:

		# set up alphabet page
		alphabet_url  = root_url + alphabet_suburl
		alphabet_page = requests.get(alphabet_url)
		alphabet_soup = BeautifulSoup(alphabet_page.content, 'lxml')

		print
		print alphabet_url
		print

		# crawl the alphabet page
		for alphabet_link in alphabet_soup.find_all('a'):

			# filter out links without 'artists' in them
			if 'artists' in alphabet_link.get('href'):
			
				# the rest are subalphabet links
				subalphabet_url = alphabet_link.get('href')

				# skip links already accumulated
				if subalphabet_url in subalphabet_urls: continue

				# accumulate links
				print '\t', subalphabet_url
				subalphabet_urls.append(subalphabet_url)

for url in subalphabet_urls: print url
