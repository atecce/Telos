#!/usr/bin/env python
#
# I should not like my writing to spare other people the trouble of thinking. 
# But, if possible, to stimulate someone to thoughts of their own.
#

# need this to make canvas
import os

# need these to get and parse html
from bs4 import BeautifulSoup
import requests
import re

# set up main page
root_url  = 'http://www.lyrics.net/'
main_page = requests.get(root_url)
main_soup = BeautifulSoup(main_page.content)

# keep these to skip collected urls
subalphabet_suburls = list()
artist_urls 	    = list()

# crawl the main page
for link in main_soup.find_all('a'):

	# filter out unwanted links
	if '.php' in link.get('href'): continue 
	if 'http' in link.get('href'): continue

	# the rest are alphabet suburls
	alphabet_suburl = link.get('href')

	# only get links relevant to artists
	if 'artists' in alphabet_suburl:

		# set up alphabet page
		alphabet_url  = root_url + alphabet_suburl
		alphabet_page = requests.get(alphabet_url)
		alphabet_soup = BeautifulSoup(alphabet_page.content)

		print
		print alphabet_url
		print

		# crawl the alphabet page
		for alphabet_link in alphabet_soup.find_all('a'):

			# filter out links without 'artists' in them
			if 'artists' in alphabet_link.get('href'):
			
				# the rest are subalphabet links
				subalphabet_suburl = alphabet_link.get('href')

				# skip links already accumulated
				if subalphabet_suburl in subalphabet_suburls: continue

				# accumulate links
				subalphabet_suburls.append(subalphabet_suburl)

				print
				print '\t', subalphabet_suburl
				print

				# set up subalphabet page	
				subalphabet_page = requests.get(root_url + subalphabet_suburl)
				subalphabet_soup = BeautifulSoup(subalphabet_page.content)

				# crawl the subalphabet page
				for subalphabet_link in subalphabet_soup.find_all('a'):

					# filter out unwanted links
					if re.match(r'^/.*', subalphabet_link.get('href')): continue

					if 'http' 	in subalphabet_link.get('href'): continue 
					if '.php' 	in subalphabet_link.get('href'): continue
					if 'javascript' in subalphabet_link.get('href'): continue

					if root_url + subalphabet_link.get('href') == root_url: continue

					# remaining results are artist urls
					artist_url = root_url + subalphabet_link.get('href')

					# skip links already accumulated
					if artist_url in artist_urls: continue

					# get canvas from url
					priest = re.match(r'.*(artist/.*)/.*', artist_url).group(1)
					print '\t\t', priest
					print

					# accumulate links
					artist_urls.append(artist_url)

					# get up artist page page
					artist_page = requests.get(artist_url)
					artist_soup = BeautifulSoup(artist_page.content)

					# crawl the artist page
					for item in artist_soup.find_all('h3'):

						# get all the links
						artist_links = item.find_all('a')

						# make sure there are more than zero
						if artist_links:

							for artist_link in artist_links:

								# filter out unwanted links
								if 'artist'     in artist_link.get('href'): continue
								if 'javascript' in artist_link.get('href'): continue

								# create a canvas for our artist
								if not os.path.exists(priest): os.makedirs(priest)

								# remaining results are albums
								album_title = artist_link.text
								album_url   = root_url + artist_link.get('href')

								# begin to conduct mass
								mass = priest + '/' + album_title

								print '\t\t\t', album_title
								print '\t\t\t', album_url
								print

								# set up artist page
								album_page = requests.get(album_url)
								album_soup = BeautifulSoup(album_page.content)

								# get all the links
								album_links = album_soup.find_all('a') 

								# crawl the album page
								for album_link in album_links: 

									# filter out unwanted links
									if re.match(r'^/lyric', album_link.get('href')):

										# start to conduct mass
										if not os.path.exists(mass): os.makedirs(mass)

										# remaining results are songs
										song_title = album_link.text
										song_url   = root_url + album_link.get('href')

										print '\t\t\t\t', song_title
										print '\t\t\t\t', song_url
										print

										# set up song page
										song_page = requests.get(song_url)
										song_soup = BeautifulSoup(song_page.content)

										# the holy text
										lyrics = song_soup.find_all('pre')[0].text

										prayer = mass + '/' + song_title

										with open(prayer, 'w') as f: f.write(lyrics)

										for line in lyrics.splitlines():

											print '\t\t\t\t\t', line

										print

								print

					print
