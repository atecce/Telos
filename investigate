#!/usr/bin/env python
#
# I should not like my writing to spare other people the trouble of thinking. 
# But, if possible, to stimulate someone to thoughts of their own.
#

# need this to make canvas
import MySQLdb

canvas = MySQLdb.connect('localhost', 'root', '', 'telos')
brush  = canvas.cursor()

# need these to get and parse html
from bs4 import BeautifulSoup
import requests
import urllib
import re

# not proud
from timestamp import song_length_to_time

# need this to communicate
def communicate(url):

	while True:

		try:

			# get soup
			page = requests.get(url)
			soup = BeautifulSoup(page.content)

			return soup

		# give communication a chance
		except requests.exceptions.ConnectionError: pass

# set up main page
root_url  = 'http://www.lyrics.net/'
main_soup = communicate(root_url)

# keep these to skip collected urls
subalphabet_suburls = list()
artist_urls 	    = list()

missing_urls = ['http://www.lyrics.net//album/994412']

# crawl the main page
for link in main_soup.find_all('a'):

	# filter out unwanted links
	if '.php' in link.get('href'): continue 
	if 'http' in link.get('href'): continue

	# the rest are alphabet suburls
	alphabet_suburl = link.get('href')

	# only get links relevant to artists
	if 'artists' in alphabet_suburl:

		# get alphabet soup
		alphabet_url  = root_url + alphabet_suburl
		alphabet_soup = communicate(alphabet_url)

		print
		print alphabet_url
		print

		# crawl the alphabet page
		for alphabet_link in alphabet_soup.find_all('a'):

			# filter out links without 'artists' in them
			if 'artists' in alphabet_link.get('href'):
			
				# the rest are subalphabet links
				subalphabet_suburl = alphabet_link.get('href')

				# skip links already accumulated
				if subalphabet_suburl in subalphabet_suburls: continue

				# accumulate links
				subalphabet_suburls.append(subalphabet_suburl)

				print '\t', subalphabet_suburl
				print

				# get subalphabet soup
				subalphabet_soup = communicate(root_url + subalphabet_suburl)

				# crawl the subalphabet page
				for subalphabet_link in subalphabet_soup.find_all('a'):

					# filter out unwanted links
					if re.match(r'^/.*', subalphabet_link.get('href')): continue

					if 'http' 	in subalphabet_link.get('href'): continue 
					if '.php' 	in subalphabet_link.get('href'): continue
					if 'javascript' in subalphabet_link.get('href'): continue

					if root_url + subalphabet_link.get('href') == root_url: continue

					# remaining results are artist urls
					artist_url = root_url + subalphabet_link.get('href')

					# skip links already accumulated
					if artist_url in artist_urls: continue

					# get priest from url
					priest = urllib.unquote_plus(re.match(r'.*/(.*)/.*', artist_url).group(1))
					print '\t\t', priest
					print

					# remember the priest's name
					brush.execute("insert into artists (name) \
						       values (%s) \
						       on duplicate key update \
						       name=name", 
						       [priest])

					canvas.commit()
					
					# accumulate links
					artist_urls.append(artist_url)

					# get artist_soup
					artist_soup = communicate(artist_url)

					# crawl the artist page
					for header in artist_soup.find_all('h3'):

						# get all the links
						artist_links = header.find_all('a')

						for artist_link in artist_links:

							# filter out unwanted links
							if 'artist'     in artist_link.get('href'): continue
							if 'javascript' in artist_link.get('href'): continue

							# remaining results are albums
							mass 	    = artist_link.text
							album_url   = root_url + artist_link.get('href')

							# skip missing albums
							if album_url in missing_urls: continue

							print '\t\t\t', album_url
							print '\t\t\t', "Album title:", mass

							# get album soup
							album_soup = communicate(album_url)

							# find the year the work was ready
							try: 		   album_year = album_soup.find_all('div', {'class': 'album-meta'})[0].find_all('dd')[-1].text
							except IndexError: album_year = None

							# sometimes empty strings
							if album_year == '': album_year = None

							print '\t\t\t', "Album year:", album_year
							print

							# remember the work
							brush.execute("insert into albums (artist_name, title, year) \
								       values (%s, %s, %s) \
								       on duplicate key update \
								       artist_name=artist_name, title=title, year=year", 
								       [priest, mass, album_year])

							canvas.commit()

							# get all the album tracks
							tracks = album_soup.find_all('tr') 

							# crawl the album page
							for track in tracks[1:]: 

								# extract the work
								track_data = track.find_all()

								track_number = track_data[0].text
								prayer       = track_data[1].text
								track_length = track_data[-2].text

								# non-breaking spaces
								if track_length == u'\xa0': track_length = None

								# time format
								else: track_length = song_length_to_time(track_length)

								track_suburl = track_data[-3].get('href')

								print '\t\t\t\t', "Track number:", track_number
								print '\t\t\t\t', "Track title:",  prayer
								print '\t\t\t\t', "Track length:", track_length

								# for that which is accounted for
								if track_suburl:

									print '\t\t\t\t', track_suburl
									print

									# set track url
									track_url = root_url + track_suburl

									# set up track page page
									track_soup = communicate(track_url)
									
									# the holy text
									try: lyrics = track_soup.find_all('pre')[0].text

									# some of it has not yet been written
									except IndexError: lyrics = None

									if lyrics:

										# please kneel
										for line in lyrics.splitlines(): print '\t\t\t\t\t', line

									print

									# remember the work
									brush.execute("insert into songs (album_title, track, title, length, lyrics) \
										       values (%s, %s, %s, %s, %s) \
										       on duplicate key update \
										       album_title=album_title, track=track, title=title, length=length, lyrics=lyrics", 
										       [mass, track_number, prayer, track_length, lyrics])

									canvas.commit()

								print
