#!/usr/bin/env python
#
# I should not like my writing to spare other people the trouble of thinking. 
# But, if possible, to stimulate someone to thoughts of their own.
#

# need this to make canvas
import os
import MySQLdb

canvas = MySQLdb.connect('localhost', 'root', '', 'telos')
brush  = canvas.cursor()

# need these to get and parse html
from bs4 import BeautifulSoup
import requests
import urllib
import re

# set up main page
root_url  = 'http://www.lyrics.net/'
main_page = requests.get(root_url)
main_soup = BeautifulSoup(main_page.content)

# keep these to skip collected urls
subalphabet_suburls = list()
artist_urls 	    = list()

# crawl the main page
for link in main_soup.find_all('a'):

	# filter out unwanted links
	if '.php' in link.get('href'): continue 
	if 'http' in link.get('href'): continue

	# the rest are alphabet suburls
	alphabet_suburl = link.get('href')

	# only get links relevant to artists
	if 'artists' in alphabet_suburl:

		# set up alphabet page
		alphabet_url  = root_url + alphabet_suburl
		alphabet_page = requests.get(alphabet_url)
		alphabet_soup = BeautifulSoup(alphabet_page.content)

		print
		print alphabet_url
		print

		# crawl the alphabet page
		for alphabet_link in alphabet_soup.find_all('a'):

			# filter out links without 'artists' in them
			if 'artists' in alphabet_link.get('href'):
			
				# the rest are subalphabet links
				subalphabet_suburl = alphabet_link.get('href')

				# skip links already accumulated
				if subalphabet_suburl in subalphabet_suburls: continue

				# accumulate links
				subalphabet_suburls.append(subalphabet_suburl)

				print
				print '\t', subalphabet_suburl
				print

				# set up subalphabet page	
				subalphabet_page = requests.get(root_url + subalphabet_suburl)
				subalphabet_soup = BeautifulSoup(subalphabet_page.content)

				# crawl the subalphabet page
				for subalphabet_link in subalphabet_soup.find_all('a'):

					# filter out unwanted links
					if re.match(r'^/.*', subalphabet_link.get('href')): continue

					if 'http' 	in subalphabet_link.get('href'): continue 
					if '.php' 	in subalphabet_link.get('href'): continue
					if 'javascript' in subalphabet_link.get('href'): continue

					if root_url + subalphabet_link.get('href') == root_url: continue

					# remaining results are artist urls
					artist_url = root_url + subalphabet_link.get('href')

					# skip links already accumulated
					if artist_url in artist_urls: continue

					# get canvas from url
					priest = urllib.unquote_plus(re.match(r'.*/(.*)/.*', artist_url).group(1))
					print '\t\t', priest
					print

					# accumulate links
					artist_urls.append(artist_url)

					# get up artist page page
					artist_page = requests.get(artist_url)
					artist_soup = BeautifulSoup(artist_page.content)

					# crawl the artist page
					for header in artist_soup.find_all('h3'):

						# get all the links
						artist_links = header.find_all('a')

						for artist_link in artist_links:

							# filter out unwanted links
							if 'artist'     in artist_link.get('href'): continue
							if 'javascript' in artist_link.get('href'): continue

							# remaining results are albums
							album_title = re.sub('/', r'\\', artist_link.text).encode('utf8')
							album_url   = root_url + artist_link.get('href')

							print '\t\t\t', album_title
							print '\t\t\t', album_url

							# set up album page
							album_page = requests.get(album_url)
							album_soup = BeautifulSoup(album_page.content)

							# find the year the work was ready
							album_year = album_soup.find_all('div', {'class': 'album-meta'})[0].find_all('dd')[-1].text

							print '\t\t\t', album_year
							print

							# get all the album tracks
							tracks = album_soup.find_all('tr') 

							# crawl the album page
							for track in tracks[1:]: 

								track_data = track.find_all()

								track_number = track_data[0].text
								track_title  = track_data[1].text
								track_length = track_data[-2].text

								track_suburl = track_data[-3].get('href')

								print '\t\t\t\t', track_number
								print '\t\t\t\t', track_title
								print '\t\t\t\t', track_length

								if track_suburl:

									print '\t\t\t\t', track_suburl
									print

									# remaining results are songs
									track_url   = root_url + track_suburl

									# set up song page
									track_page = requests.get(track_url)
									track_soup = BeautifulSoup(track_page.content)
									
									# the holy text
									try:

										lyrics = track_soup.find_all('pre')[0].text.encode('utf8')

									# some of it has not yet been written
									except IndexError: continue

									# please kneel
									for line in lyrics.splitlines():

										print '\t\t\t\t\t', line

									print

								print
